## 感知机

**感知机是什么**

![image-20220926191325814](http://pic.shixiaocaia.fun/202209261913508.png)

> 这是一个已接受两个输入信号的感知机。
>
> 神经元、节点：图中的圈
>
> 感知机的信号，0 对应不传递的信号， 1 对应传递信号。
>
> 界限值/阈值：用$\theta$表示。
>
> 偏置b：$-\theta$。控制神经元单元被激活的难易程度。
>
> 用公式表示感知机：
>
> ![image-20220927192052756](http://pic.shixiaocaia.fun/202209272016969.png)
>
> 重定义为，也称h(x)为激活函数：
>
> ![image-20220927192410664](http://pic.shixiaocaia.fun/202209271924901.png)

- 输入信号 x 被送往神经元，再乘以固定的权重 w, 神经元会计算传送过来的信号综合 y ， y 大于特定值时才会输出 1 ，此时称为神经元**被**激活。

> 每个输入信号的权重w类似电路中电阻，控制着信号流动程度。权重越大，该信号的重要性就越强。

**感知机实现逻辑电路**

通过改变权值和阈值，实现与门、与非门、或门。

> 但是无法用一条直线实现异或门。由此推出了**单层**感知机的局限性：只能通过一条曲线分割空间，曲线无法用感知机表示。单层感知机只能表示线性空间。
>
> 但不要灰心，我们可以通过叠加实现。

**多层感知机**

由叠加层实现异或门，引出了什么是多层感知机。

多层感知机可以实现非线性空间。

多层感知机牛逼到可以实现计算机。

## 神经网络

在上面谈到通过调正感知机的权重，实现了逻辑电路，但这都是通过人工决定。

神经网络的出现就是为了解决上述问题。神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。

**神经网络的例子**

![image-20220927191553905](http://pic.shixiaocaia.fun/202209271915695.png)

> 第0层：输入层。
>
> 第1层：中间层也称为隐藏层，隐藏是指隐藏的神经元。
>
> 第2层：输出层。
>
> 由于实际只有两层神经网络有权重，因此称为2层网络。也有书根据三层网络，称之为3层网络。

### 激活函数

在上方我们已经见识到了激活函数，他是将输入信号的总和转换为输出信号的函数。

激活函数分为很多种，对于这里的感知机使用了阶跃函数，即一旦输入超过阈值，就切换输出。

**sigmoid函数**

![image-20220927193009185](http://pic.shixiaocaia.fun/202209271930468.png)

```python
>>> x = np.array([-1.0, 1.0, 2.0])
>>> sigmoid(x)
```

**ReLU函数**

![image-20220927194327948](http://pic.shixiaocaia.fun/202209271943976.png)

```python
def relu(x): 
    return np.maximum(0, x)
```

> 神经网络的激活函数必须使用非线性函数。
>
> 换句话说，激活函数不能使用线性函数。因为使用线性函数的话，加深神经网络的层数就没有意义了,多次的线性激活函数，可能等价于一次的效果。

**认识权重的符号**

![image-20220927195530798](http://pic.shixiaocaia.fun/202209271955095.png)

![image-20220927200058980](http://pic.shixiaocaia.fun/202209272001702.png)

> 隐藏层的加权和（加权信号和偏置的总和）用a表示。
>
> 被激活函数转换后的信号用z表示。
>
> 图中h()表示激活函数

### 输出层的设计

神经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出层的激活函数。一般而言，回归问题用恒等函数，分类问题用softmax函数。

> 机器学习的问题大致可以分为分类问题和回归问题。分类问题是数 据属于哪一个类别的问题。比如，区分图像中的人是男性还是女性 的问题就是分类问题。而回归问题是根据某个输入预测一个（连续的） 数值的问题。比如，根据一个人的图像预测这个人的体重的问题就
> 是回归问题。

**恒等函数**

输入是什么，输出就是什么，不多说。

**softmax函数**

![image-20220927201127472](http://pic.shixiaocaia.fun/202209272011428.png)

分子是输入信号a<sub>k</sub>的指数函数，分母是所有输入信号的指数函数的和。

考虑到次数过大时，softmax可能导致溢出问题，对softmax进行优化得到一下：

![image-20221006223952586](http://pic.shixiaocaia.fun/202210062239240.png)

**softmax函数的特征**

softmax函数的输出值的总和是1。将softmax函数的输出解释为“ 概率 ”。

由于softmax函数中的指数函数是递增函数，所以使用softmax函数之后各个元素的大小关系也不会改变。

一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。
并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，
**神经网络在进行分类时，输出层的softmax函数可以省略**。在实际的问题中，
由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数
一般会被省略。

**输出层的神经元数量**

一般根据待解决的问题来决定。

对于分类问题，输出层的神经元数量一般设定为类别的数量。

## 神经网络的学习

### 训练数据和测试数据

用训练数据来进行学习，寻找最优的参数，训练数据也称为监督数据；

使用测试数据评价训练得到的模型的实际能力。

> 将二者分开，是为了追求模型的泛化能力，正确评价模型。
>
> 同时也要考虑过拟合的问题。

### 损失函数

寻找最优权重的参数。表征了神经网络对监督数据在多大程度上不拟合，多大程度上不一致。

计算损失函数时必须将所有的训练数据作为对象。

> 加上一个符号，就变成了性能有多好，多大程度上不坏。
>
> 损失函数值越小，表明和监督数据之间误差小，更加吻合准确。

**均方误差**

![image-20221007093243070](http://pic.shixiaocaia.fun/202210070932237.png)

![image-20221007093356234](http://pic.shixiaocaia.fun/202210070933260.png)

**交叉熵误差**

![image-20221007094615087](http://pic.shixiaocaia.fun/202210070946112.png)

![image-20221007094811168](http://pic.shixiaocaia.fun/202210070948415.png)

实际上只计算了正确解标签所对应的输出结果。

根据logx的图像，我们知道，正确解的输出越大，越接近0，交叉熵的误差越接近0。

**为什么要设定损失函数**

对于数字识别任务，如果以识别精度作为指标，参数的导数在绝大部分会变成0，仅仅微调参数，不会发生变化或者不会发生连续的变化。

### 数值微分

在计算机中实现数值微分，或者说是导数，是会产生舍入误差，以及由于真的导数（依据数学求导公式）是在某一个点处的，而我们计算是差值变化得到的。

>舍入误差（roundingerror）。所谓舍人误差，是指因省略小数的精细部分的数值（比如，小数点第8位以后的数值）而造成最终的计算结果上的误差。

> 中心差分，以x为中心，计算它左右两边的差分。( f(x + h) - f(x - h) ) / 2h

**梯度法**

梯度会指向各点处的函数值降低的方向，梯度指示的方向是各点处的函数值减小最多的方向。

机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必
须在学习时找到最优参数（权重和偏置）。这里所说的最优参数是指损失函数取最小值的情况。

**学习率**

![image-20221007111512136](http://pic.shixiaocaia.fun/202210071115561.png)

增加了一个参数，学习率。这是超参数中的一种，这类超参数都是人为设定的。

### 随机梯度下降法

1. 从训练数据中随机选出一部分数据，mini-batch，目标减小mini-batch。
2. 计算各个权重参数的梯度。
3. 将权重参数沿梯度方向进行微小变化。
4. 重复1，2，3步骤。

